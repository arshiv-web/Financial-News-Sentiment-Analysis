# AI-Assisted Task: Financial Market Movement Prediction

## Project Overview
This repository contains the code and data for Assignment 2 of CSE 594. The project implements and evaluates an AI model designed to assist human analysts in predicting S&P 500 market movements based on daily financial news headlines and contextual market indicators.

---

## Contents
- **`Assignment_Report.pdf`**: The main project report detailing the task design, model implementation, evaluation, and analysis.
- **`results/checkpoint-10350`**: Best Trained Version of the model checkpointed.

- **`dataPreProcessing.ipynb`**: A Jupyter Notebook that performs all data sourcing, cleaning, feature engineering, and dataset splitting.
- **`modelTraining.ipynb`**: A Jupyter Notebook that handles the complete model fine-tuning process using a custom `WeightedTrainer` and evaluates the final model.
- **`predictStudyDataset.ipynb`**: A jupyter notebook that lets you do inference on the study dataset using the best trained version of the model.
- **`predictNew.ipynb`**: A jupyter notebook that lets you do inference on any new data using the best trained version of the model.

- **`study_dataset.csv`**: The final 200-trial study dataset, including the input text and ground truth label, generated by `dataPreProcessing.ipynb`.
- **`study_dataset_with_predictions.csv`**: The final 200-trial study dataset, including the input text, ground truth label, the AI's prediction, and a column indicating correctness. This is the primary file for the final evaluation and error analysis.
- **`training_dataset.csv` & `validation_dataset.csv`**: The datasets used for training and validating the model, generated by `dataPreProcessing.ipynb`.
- **`sp500_headlines_2008_2024.csv`**: Pre-processed data
---

## Setup and Requirements
The project is designed to be run in a **Google Colab environment** with a T4 GPU runtime.

The required Python libraries will be installed automatically by the Jupyter Notebooks. They include:
pandas
numpy
yfinance
scikit-learn
transformers
datasets
torch

---

## How to Evaluate and Replicate

The notebooks are designed to be run directly in Google Colab. The model checkpoint and all necessary data are included in this submission folder.

### Quick Evaluation (Primary Workflow)

This workflow allows you to see the final results and test the model without retraining. Make sure to upload the model checkpoint to the environment.

1.  **Evaluate on Study Dataset:**
    - Open **`predictStudyDataset.ipynb`**.
    - Run all cells. This notebook will load the pre-trained model from the `/results/checkpoint-10350` directory and generate the final classification report on the `study_dataset.csv`. Please make sure you have the relevant files uploaded to the environment.

2.  **Test with New Headlines:**
    - Open **`predictNew.ipynb`**.
    - Modify the text cell containing the `new_data` list with any custom headlines and context you want to test.
    - Run all cells to see the live predictions from the best model. Please make sure you have the relevant files uploaded to the environment.

### Full Replication (Optional)

If you wish to replicate the entire project from scratch:

1.  **Pre-process the Data:**
    - Open **`dataPreProcessing.ipynb`**.
    - Ensure `sp500_headlines_2008_2024.csv` is in the same directory.
    - Run all cells to regenerate the `training_dataset.csv`, `validation_dataset.csv`, and `study_dataset.csv`.

2.  **Retrain the Model:**
    - Open **`modelTraining.ipynb`**.
    - Ensure the datasets from the previous step are present.
    - Run all cells to fine-tune the model from scratch. This will create a new set of checkpoints in the `/results` directory. *Note: This process takes a significant amount of time.*

---

For any questions, please contact Shivam Arora ([arshiv@umich.edu](mailto:arshiv@umich.edu)).